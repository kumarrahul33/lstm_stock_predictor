{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA Segment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scaler import StandardScalerLSTM\n",
    "\n",
    "\n",
    "# Gym stuff\n",
    "import gym\n",
    "import gym_anytrading\n",
    "\n",
    "# Stable baselines - rl stuff\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines.common.policies import FeedForwardPolicy\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "\n",
    "SEQ_LEN = 8\n",
    "BATCH_SIZE = 16\n",
    "FEATURE_SIZE = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/kanad/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.12.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow-gpu==1.15.0\u001b[0m\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! You can use the GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available! You can use the GPU.\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"CUDA is not available. You'll be using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"data/final_dataset.csv\")\n",
    "\n",
    "data_df = data_df.drop(['Date'], axis=1)\n",
    "data_df = data_df.iloc[1900:,:]\n",
    "data = np.nan_to_num(np.array(data_df, dtype=np.float32))\n",
    "\n",
    "# ====================== Scaler ========================\n",
    "\n",
    "# Scaler\n",
    "minmax_scaler = StandardScalerLSTM(batch_size=BATCH_SIZE, sequence_length=SEQ_LEN, feature_size=FEATURE_SIZE,device=device)\n",
    "minmax_scaler.fit(data)\n",
    "\n",
    "# ====================== Scaler ========================\n",
    "def create_sequence(data,seq_len):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data)-seq_len-1):\n",
    "        x = data[i:(i+seq_len),:]\n",
    "        # print(x.shape)\n",
    "        y = data[i+seq_len,0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs),np.array(ys)\n",
    "\n",
    "inputs , targets = create_sequence(data,SEQ_LEN)\n",
    "inputs=torch.from_numpy(inputs);targets=torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3542, 8, 12]) torch.Size([394, 8, 12])\n"
     ]
    }
   ],
   "source": [
    "# split the input data into train and test data\n",
    "train_size = int(0.9 * len(inputs))\n",
    "test_size = len(inputs) - train_size\n",
    "train_inputs, test_inputs = inputs[:train_size], inputs[train_size:]\n",
    "train_targets, test_targets = targets[:train_size], targets[train_size:]\n",
    "print(train_inputs.shape, test_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== Model ======================== \n",
    "class PricePredictor(nn.Module):\n",
    "    def __init__(self,scaler,batch_size, input_size=12, hidden_layer_size=150, time_segment=5, output_size=1):\n",
    "        super().__init__()\n",
    "        self.scaler = scaler\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.time_segment_length = time_segment\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size,batch_first=True) # N x L x input_size(12)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size) # N x L x output_size(1)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        # cell double is zeros vector of shape ((1, batch_size, hidden_layer_size), (1, batch_size, hidden_layer_size))\n",
    "        self.cell_double =   (torch.zeros(1,batch_size,hidden_layer_size,requires_grad=False).to(device),\n",
    "                              torch.zeros(1,batch_size,hidden_layer_size,requires_grad=False).to(device))\n",
    "        self.remember = False \n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        '''\n",
    "            input : N x L x input_size\n",
    "            ouput : N x output_size\n",
    "        '''\n",
    "        input_seq = self.scaler(input_seq)\n",
    "\n",
    "        if self.remember:\n",
    "            output,self.cell_double = self.lstm(input_seq,self.cell_double) # N x L x hidden_layer_size\n",
    "        else:\n",
    "            output,_ = self.lstm(input_seq) # N x L x hidden_layer_size\n",
    "        output = self.ReLU(output)\n",
    "        self.cell_double = (self.cell_double[0].detach(),self.cell_double[1].detach())\n",
    "        predictions = self.linear(output[:,-1,:].squeeze()) # N x output_size\n",
    "        predictions = self.scaler.inverse_transform(predictions)\n",
    "        return predictions, output[:,-1,:].squeeze()\n",
    "\n",
    "# ====================== Model ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(losses, patience=3):\n",
    "    return patience < len(losses) and all([losses[-i-1] > losses[-i-2] for i in range(patience)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDQNPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomDQNPolicy, self).__init__(*args, **kwargs,\n",
    "                                              net_arch=[32, 32],  # Adjust layers as needed\n",
    "                                              feature_extraction=\"custom\")\n",
    "        # Initialize LSTM Model\n",
    "        self.lstm_model = PricePredictor(input_size=obs_size, hidden_layer_size=hidden_state_size)\n",
    "        # Load trained LSTM model here if you have one\n",
    "\n",
    "    def extract_features(self, obs):\n",
    "        # Process observation through LSTM and extract hidden state\n",
    "        hidden_state = self.lstm_model(obs)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningModel(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, num_actions):\n",
    "        super(QLearningModel, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_layer_size, num_actions)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        return self.fc(hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def calculate_reward(action, current_price, previous_price, holding_status, transaction_cost=0.01):\n",
    "    if action == 1:  # Buy\n",
    "        if holding_status == 0:  # If not already holding\n",
    "            reward = -transaction_cost  # Pay transaction cost\n",
    "            print(\"temp_reward\", reward)\n",
    "            holding_status = 1  # Update holding status\n",
    "        else:\n",
    "            reward = 0  # No action taken, no reward\n",
    "\n",
    "    elif action == 2:  # Sell\n",
    "        if holding_status == 1:  # If holding a stock\n",
    "            profit = current_price - previous_price\n",
    "            reward = profit - (transaction_cost * current_price)  # Gain from selling minus transaction cost \n",
    "            print(\"temp_reward\", reward)\n",
    "            holding_status = 0  # Update holding status\n",
    "        else:\n",
    "            reward = 0  # No action taken, no reward\n",
    "\n",
    "    else:  # Hold\n",
    "        if holding_status == 1:  # If holding a stock\n",
    "            reward = current_price - previous_price  # Unrealized gain/loss\n",
    "            print(\"temp_reward\", reward)\n",
    "        else:\n",
    "            reward = 0  # No stock held, no reward\n",
    "\n",
    "    return reward, holding_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "temp_reward -0.01\n",
      "rewards tensor([[-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100],\n",
      "        [-0.0100]])\n",
      "holding_status tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32307/1470495071.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ql_loss = torch.tensor(rewards)  # Placeholder for actual Q-learning loss calculation\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/kanad/aiml_project/copy_model_test.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/kanad/aiml_project/copy_model_test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# ql_loss = torch.tensor(rewards).to(device)  # Placeholder for actual Q-learning loss calculation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/kanad/aiml_project/copy_model_test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     ql_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(rewards)  \u001b[39m# Placeholder for actual Q-learning loss calculation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/kanad/aiml_project/copy_model_test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     ql_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/kanad/aiml_project/copy_model_test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     ql_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/kanad/aiml_project/copy_model_test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m(i\u001b[39m%\u001b[39m\u001b[39m5\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/kanad/aiml_project/copy_model_test.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39m# print the test loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(train_inputs, train_targets)\n",
    "test_dataset = TensorDataset(test_inputs, test_targets)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "model = PricePredictor(minmax_scaler,BATCH_SIZE).to(device)\n",
    "ql_model = QLearningModel(hidden_layer_size=150, num_actions=3).to(device)  # 3 actions: Buy, Sell, Hold\n",
    "loss_function = nn.MSELoss()\n",
    "# loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01)\n",
    "ql_optimizer = torch.optim.Adam(ql_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 80\n",
    "test_losses = []\n",
    "test_rewards = []\n",
    "holding_status = torch.zeros(size=(BATCH_SIZE,1))\n",
    "for i in range(epochs):\n",
    "    for seq, targets in train_loader:\n",
    "        if seq.shape[0]!=BATCH_SIZE: # if the last batch is not full\n",
    "            continue\n",
    "        seq, targets = seq.to(device), targets.to(device)\n",
    "        price_predictions, hidden_states = model(seq)\n",
    "        hidden_states = hidden_states.to(device)\n",
    "        q_values = ql_model(hidden_states)\n",
    "        actions = torch.argmax(q_values, dim=1)\n",
    "        rewards = torch.zeros(size = (actions.shape[0],1))\n",
    "        print(\"actions\", actions)\n",
    "        for j in range(len(actions)):\n",
    "            rewards[j], holding_status[j] = calculate_reward(actions[j], price_predictions[j], targets[j], holding_status[j], transaction_cost=0.01)\n",
    "        print(\"rewards\", rewards)\n",
    "        print(\"holding_status\", holding_status)\n",
    "        optimizer.zero_grad()\n",
    "        single_loss = loss_function(price_predictions, targets)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        ql_optimizer.zero_grad()\n",
    "        # ql_loss = torch.tensor(rewards).to(device)  # Placeholder for actual Q-learning loss calculation\n",
    "        ql_loss = torch.tensor(rewards)  # Placeholder for actual Q-learning loss calculation\n",
    "        ql_loss.backward()\n",
    "        ql_optimizer.step()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    if(i%5==0):\n",
    "        # print the test loss\n",
    "        with torch.no_grad():\n",
    "            model.remember = True  # remember the last state of the cell\n",
    "            test_loss_ = 0\n",
    "            test_reward_ = 0\n",
    "            for seq_, targets_ in test_loader:\n",
    "                if(seq_.shape[0]!=BATCH_SIZE): # if the last batch is not full\n",
    "                    continue\n",
    "                seq_, targets_ = seq_.to(device), targets_.to(device)\n",
    "                \n",
    "                y_pred_test, hidden_states_ = model(seq_)\n",
    "                q_values_ = ql_model(hidden_states_)\n",
    "                test_loss_ += loss_function(y_pred_test, targets_)\n",
    "                test_reward_ += torch.tensor(rewards)\n",
    "                # print(y_pred_test.shape)\n",
    "            print(f'Test loss: {test_loss_.item():10.8f}')\n",
    "            print(f'Test loss: {test_reward_.item():10.8f}')\n",
    "            test_losses.append(test_loss_.item())\n",
    "            test_rewards.append(test_reward_.item())\n",
    "            if early_stopping(test_losses, patience=3):\n",
    "                break \n",
    "            model.remember = False # forget the last state of the cell for further training\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== Plot ========================\n",
    "model.remember = True\n",
    "actual_prices = []\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    for seq, targets in test_loader:\n",
    "        seq, targets = seq.to(device), targets.to(device)\n",
    "        if seq.shape[0]!=BATCH_SIZE: # if the last batch is not full\n",
    "            continue\n",
    "        y_pred, hidden_states_ = model(seq)\n",
    "        targets = targets.cpu().numpy()\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        actual_prices.extend(targets.flatten())\n",
    "        predictions.extend(y_pred.flatten())\n",
    "    \n",
    "\n",
    "\n",
    "actual_prices = np.array(actual_prices)\n",
    "predictions = np.array(predictions)\n",
    "x = np.arange(len(actual_prices))\n",
    "\n",
    "plt.plot(x,actual_prices, label=\"actual\")\n",
    "plt.plot(x,predictions, label=\"predictions\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
