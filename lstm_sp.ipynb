{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>MACD</th>\n",
       "      <th>ATR</th>\n",
       "      <th>RSI</th>\n",
       "      <th>EFFR</th>\n",
       "      <th>VIX</th>\n",
       "      <th>USDX</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>UMCSENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1498.13</td>\n",
       "      <td>1508.18</td>\n",
       "      <td>1497.56</td>\n",
       "      <td>1498.13</td>\n",
       "      <td>-3.311325</td>\n",
       "      <td>17.195714</td>\n",
       "      <td>57.426836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.469999</td>\n",
       "      <td>111.790001</td>\n",
       "      <td>4.1</td>\n",
       "      <td>107.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1505.97</td>\n",
       "      <td>1507.30</td>\n",
       "      <td>1489.39</td>\n",
       "      <td>1505.97</td>\n",
       "      <td>-2.269902</td>\n",
       "      <td>17.627857</td>\n",
       "      <td>60.150376</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.379999</td>\n",
       "      <td>111.419998</td>\n",
       "      <td>4.1</td>\n",
       "      <td>107.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1508.31</td>\n",
       "      <td>1511.16</td>\n",
       "      <td>1501.30</td>\n",
       "      <td>1508.31</td>\n",
       "      <td>-1.556307</td>\n",
       "      <td>17.228571</td>\n",
       "      <td>63.012140</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.040001</td>\n",
       "      <td>111.230003</td>\n",
       "      <td>4.1</td>\n",
       "      <td>107.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1506.45</td>\n",
       "      <td>1513.41</td>\n",
       "      <td>1505.16</td>\n",
       "      <td>1506.45</td>\n",
       "      <td>-0.873683</td>\n",
       "      <td>18.557143</td>\n",
       "      <td>67.127444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.530001</td>\n",
       "      <td>111.349998</td>\n",
       "      <td>4.1</td>\n",
       "      <td>107.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1514.09</td>\n",
       "      <td>1523.88</td>\n",
       "      <td>1506.46</td>\n",
       "      <td>1514.09</td>\n",
       "      <td>0.124412</td>\n",
       "      <td>20.516429</td>\n",
       "      <td>76.296296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.540001</td>\n",
       "      <td>111.459999</td>\n",
       "      <td>4.1</td>\n",
       "      <td>107.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High      Low    Close      MACD        ATR        RSI EFFR  \\\n",
       "0  1498.13  1508.18  1497.56  1498.13 -3.311325  17.195714  57.426836  NaN   \n",
       "1  1505.97  1507.30  1489.39  1505.97 -2.269902  17.627857  60.150376  NaN   \n",
       "2  1508.31  1511.16  1501.30  1508.31 -1.556307  17.228571  63.012140  NaN   \n",
       "3  1506.45  1513.41  1505.16  1506.45 -0.873683  18.557143  67.127444  NaN   \n",
       "4  1514.09  1523.88  1506.46  1514.09  0.124412  20.516429  76.296296  NaN   \n",
       "\n",
       "         VIX        USDX  UNRATE  UMCSENT  \n",
       "0  17.469999  111.790001     4.1    107.3  \n",
       "1  17.379999  111.419998     4.1    107.3  \n",
       "2  17.040001  111.230003     4.1    107.3  \n",
       "3  16.530001  111.349998     4.1    107.3  \n",
       "4  16.540001  111.459999     4.1    107.3  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"data/final_dataset.csv\")\n",
    "\n",
    "data_df = data_df.drop(['Date'], axis=1)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1498.13 1508.18 1497.56 ...  111.79    4.1   107.3 ]\n",
      " [1505.97 1507.3  1489.39 ...  111.42    4.1   107.3 ]\n",
      " [1508.31 1511.16 1501.3  ...  111.23    4.1   107.3 ]\n",
      " ...\n",
      " [1283.57 1286.62 1267.11 ...  118.7     4.5    92.6 ]\n",
      " [1270.03 1283.85 1269.03 ...  119.36    4.5    92.6 ]\n",
      " [1276.96 1277.08 1265.08 ...  119.09    4.5    92.6 ]]\n"
     ]
    }
   ],
   "source": [
    "# data_df.index\n",
    "data_df = data_df.iloc[0:200,:]\n",
    "data_df.index\n",
    "data = np.array(data_df, dtype=np.float32)\n",
    "# replace nans with 0\n",
    "data = np.nan_to_num(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22525/306952755.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  data[:,i] = (data[:,i] - min_val) / (max_val - min_val)\n"
     ]
    }
   ],
   "source": [
    "def scale_data(data):\n",
    "    num_features = data.shape[1]\n",
    "    scale_params = np.zeros((num_features,2))\n",
    "    for i in range(num_features):\n",
    "        min_val = np.min(data[:,i])\n",
    "        max_val = np.max(data[:,i])\n",
    "        scale_params[i,:] = [min_val, max_val]\n",
    "        data[:,i] = (data[:,i] - min_val) / (max_val - min_val)\n",
    "    return data, scale_params\n",
    "\n",
    "data, scale_params = scale_data(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 5, 12)\n",
      "(194,)\n"
     ]
    }
   ],
   "source": [
    "def create_sequence(data,seq_len):\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for i in range(len(data)-seq_len-1):\n",
    "        x = data[i:(i+seq_len),:]\n",
    "        # print(x.shape)\n",
    "        y = data[i+seq_len,0]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs),np.array(ys)\n",
    "\n",
    "inputs , targets = create_sequence(data,5)\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "# can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32,\n",
    "print(inputs.dtype)\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(inputs,targets)\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset) - int(0.8*len(dataset))])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=12, hidden_layer_size=30, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.cell_double = (torch.zeros(1,self.hidden_layer_size),\n",
    "                        torch.zeros(1,self.hidden_layer_size))\n",
    "    def forward(self, input_seq):\n",
    "        print(\"here\")\n",
    "        lstm_out, self.cell_double = self.lstm(input_seq, self.cell_double)\n",
    "        predictions = self.linear(lstm_out)\n",
    "        print(predictions)\n",
    "        return predictions[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "tensor([nan], grad_fn=<SelectBackward0>)\n",
      "here\n",
      "tensor([nan], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/rahulkumar/code/iitb_cs/ml_lab/project/lstm_stock_predictor/lstm_sp.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rahulkumar/code/iitb_cs/ml_lab/project/lstm_stock_predictor/lstm_sp.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mprint\u001b[39m(y_pred)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rahulkumar/code/iitb_cs/ml_lab/project/lstm_stock_predictor/lstm_sp.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     single_loss \u001b[39m=\u001b[39m loss_function(y_pred, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/rahulkumar/code/iitb_cs/ml_lab/project/lstm_stock_predictor/lstm_sp.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     single_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rahulkumar/code/iitb_cs/ml_lab/project/lstm_stock_predictor/lstm_sp.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/rahulkumar/code/iitb_cs/ml_lab/project/lstm_stock_predictor/lstm_sp.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch: \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m:\u001b[39;00m\u001b[39m3\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{\u001b[39;00msingle_loss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m10.8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 194 \n",
    "for i in range(epochs):\n",
    "    model.cell_double = (torch.zeros(1,model.hidden_layer_size),\n",
    "                        torch.zeros(1,model.hidden_layer_size))\n",
    "    for seq, labels in train_loader:\n",
    "        seq = seq.view(5,12)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        print(y_pred)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "    # if(i%5==0):\n",
    "\n",
    "    #     print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
